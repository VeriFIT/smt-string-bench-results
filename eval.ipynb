{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import eval_functions\n",
    "importlib.reload(eval_functions)\n",
    "from eval_functions import *\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import numpy as n\n",
    "\n",
    "# make tables interactive\n",
    "from itables import init_notebook_mode\n",
    "import itables.options as opt\n",
    "init_notebook_mode(all_interactive=True, connected=True)\n",
    "opt.maxBytes=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOODLER_VERSION=\"\"\n",
    "OLD_NOODLER_VERSION=\"983a432-56780ef\"\n",
    "if NOODLER_VERSION == \"\":\n",
    "    NOODLER_VERSION = OLD_NOODLER_VERSION\n",
    "NOODLER=f\"z3-noodler-{NOODLER_VERSION}\"\n",
    "NOODLER_ONLY_STABILIZATION=f\"z3-noodler-only-stabilization-{NOODLER_VERSION}\"\n",
    "NOODLER_STATS=f\"z3-noodler-stats-{NOODLER_VERSION}\"\n",
    "NOODLER_NO_NIELSEN_STATS=f\"z3-noodler-no-nielsen-stats-{NOODLER_VERSION}\"\n",
    "NOODLER_NO_LENGTH_STATS=f\"z3-noodler-no-length-stats-{NOODLER_VERSION}\"\n",
    "NOODLER_NO_MEMB_STATS=f\"z3-noodler-no-memb-stats-{NOODLER_VERSION}\"\n",
    "NOODLER_STATS=f\"z3-noodler-stats-{NOODLER_VERSION}\"\n",
    "NOODLER_MODEL=f\"z3-noodler-model-{NOODLER_VERSION}\"\n",
    "NOODLER_CHECK_MODEL=f\"check-model-{NOODLER_VERSION}\"\n",
    "OLD_NOODLER=f\"z3-noodler-{OLD_NOODLER_VERSION}\"\n",
    "OLD_NOODLER_MODEL=f\"z3-noodler-model-{OLD_NOODLER_VERSION}\"\n",
    "CVC5=\"cvc5-1.2.0\"\n",
    "CVC5_MODEL=\"cvc5-model-1.2.0\"\n",
    "Z3=\"z3-4.13.4\"\n",
    "Z3_MODEL=\"z3-model-4.13.0\"\n",
    "Z3STR4=\"z3str4\"\n",
    "OSTRICH=\"ostrich-70545314\"\n",
    "Z3STR3RE=\"z3strRE\"\n",
    "Z3TRAU=\"z3-trau-1.1\"\n",
    "Z3ALPHA=\"z3-alpha-smtcomp2024\"\n",
    "\n",
    "if NOODLER == \"\":\n",
    "  NOODLER = OLD_NOODLER\n",
    "\n",
    "TOOLS = list(dict.fromkeys([ # small hack so that we get list of unique values (i.e. a set, but also in the given order; see https://stackoverflow.com/questions/1653970/does-python-have-an-ordered-set)\n",
    "    # NOODLER,\n",
    "    # OLD_NOODLER,\n",
    "    CVC5,\n",
    "    Z3,\n",
    "    # NOODLER_MODEL,\n",
    "    # OLD_NOODLER_MODEL,\n",
    "    # CVC5_MODEL,\n",
    "    # Z3_MODEL,\n",
    "    # NOODLER_CHECK_MODEL,\n",
    "    # NOODLER_STATS,\n",
    "    # NOODLER_NO_NIELSEN_STATS,\n",
    "    # NOODLER_NO_LENGTH_STATS,\n",
    "    # NOODLER_NO_MEMB_STATS,\n",
    "    # NOODLER_ONLY_STABILIZATION,\n",
    "    # Z3STR4,\n",
    "    # Z3ALPHA,\n",
    "    # OSTRICH,\n",
    "    # Z3STR3RE,\n",
    "    # Z3TRAU,\n",
    "\n",
    "    # you can add more tools here directly if needed\n",
    "]))\n",
    "\n",
    "VBS = [\n",
    "    # [Z3, CVC5],\n",
    "    # [NOODLER, CVC5],\n",
    "    # [NOODLER, Z3],\n",
    "    # [NOODLER, Z3, CVC5],\n",
    "]\n",
    "\n",
    "TOOLS_STATS = [NOODLER_STATS, NOODLER_NO_NIELSEN_STATS, NOODLER_NO_LENGTH_STATS, NOODLER_NO_MEMB_STATS]\n",
    "\n",
    "\n",
    "bench_selection = (\n",
    "  # Select one:\n",
    "    # \"NORMAL\"\n",
    "    # \"INT_CONVS\"\n",
    "    # \"QF_S\"\n",
    "    # \"QF_SLIA\"\n",
    "    # \"QF_SNIA\"\n",
    "    # \"ZALIGVINDER\"\n",
    "    # \"REGEX\"\n",
    "    # \"NOTCONTAINS\"\n",
    "    \"FMBENCHMARKS\"\n",
    ")\n",
    "\n",
    "if bench_selection == \"NORMAL\":\n",
    "  BENCHES = [\n",
    "      \"sygus_qgen\",\n",
    "      \"denghang\",\n",
    "      \"automatark\",\n",
    "      \"stringfuzz\",\n",
    "      \"redos\",\n",
    "\n",
    "      \"norn\",\n",
    "      \"slog\",\n",
    "      \"slent\",\n",
    "      \"omark\",\n",
    "      \"kepler\",\n",
    "      \"woorpje\",\n",
    "      \"webapp\",\n",
    "      \"kaluza\",\n",
    "\n",
    "      \"transducer_plus\",\n",
    "      \"leetcode\",\n",
    "      \"str_small_rw\",\n",
    "      \"pyex\",\n",
    "      \"full_str_int\",\n",
    "\n",
    "      \"snia\",\n",
    "      ]\n",
    "elif bench_selection == \"INT_CONVS\":\n",
    "  # Only benchmarks with to_int/from_int\n",
    "  BENCHES = [\n",
    "      \"stringfuzz\",\n",
    "      \"str_small_rw\",\n",
    "      \"full_str_int\",\n",
    "  ]\n",
    "elif bench_selection == \"QF_S\":\n",
    "  BENCHES = [\n",
    "      \"sygus_qgen\",\n",
    "      \"automatark\",\n",
    "\n",
    "      \"slog\",\n",
    "      \"woorpje\",\n",
    "  ]\n",
    "elif bench_selection == \"QF_SLIA\":\n",
    "  BENCHES = [\n",
    "      \"denghang\",\n",
    "      \"stringfuzz\",\n",
    "\n",
    "      \"norn\",\n",
    "      \"slent\",\n",
    "      \"transducer_plus\",\n",
    "      \"kepler\",\n",
    "      \"woorpje\",\n",
    "      \"webapp\",\n",
    "      \"kaluza\",\n",
    "      \"redos\",\n",
    "\n",
    "      \"leetcode\",\n",
    "      \"str_small_rw\",\n",
    "      \"pyex\",\n",
    "      \"full_str_int\",\n",
    "  ]\n",
    "elif bench_selection == \"QF_SNIA\":\n",
    "  BENCHES = [\n",
    "    \"snia\"\n",
    "  ]\n",
    "elif bench_selection == \"ZALIGVINDER\":\n",
    "  BENCHES = [\n",
    "    \"zaligvinder\"\n",
    "  ]\n",
    "elif bench_selection == \"REGEX\":\n",
    "  BENCHES = [\n",
    "    \"regex\"\n",
    "  ]\n",
    "elif bench_selection == \"NOTCONTAINS\":\n",
    "  # benchmarks used in not contains paper (probably)\n",
    "  BENCHES = [\n",
    "    \"selected_hard\",\n",
    "    \"django\",\n",
    "    \"thefuck\",\n",
    "    \"biopython\",\n",
    "    \"notcontains\",\n",
    "  ]\n",
    "elif bench_selection == \"FMBENCHMARKS\":\n",
    "  # benchmarks used in FM'23 paper\n",
    "  BENCHES = [\n",
    "    \"pyex_hard\",\n",
    "    \"kaluza_hard\",\n",
    "    \"str_2\",\n",
    "    \"slog_old\",\n",
    "  ]\n",
    "\n",
    "REGEX_GROUP_NAME = \"regex\"\n",
    "EQUATIONS_GROUP_NAME = \"equations\"\n",
    "PREDICATES_GROUP_NAME = \"predicates\"\n",
    "\n",
    "REGEX_BENCHES = [\n",
    "    \"sygus_qgen\",\n",
    "    \"denghang\",\n",
    "    \"automatark\",\n",
    "    \"stringfuzz\",\n",
    "    \"redos\",\n",
    "]\n",
    "\n",
    "EQUATIONS_BENCHES = [\n",
    "    \"norn\",\n",
    "    \"slog\",\n",
    "    \"slent\",\n",
    "    \"omark\",\n",
    "    \"kepler\",\n",
    "    \"woorpje\",\n",
    "    \"webapp\",\n",
    "    \"kaluza\",\n",
    "    \"snia\",\n",
    "\n",
    "    \"pyex_hard\",\n",
    "    \"kaluza_hard\",\n",
    "    \"str_2\",\n",
    "    \"slog_old\",\n",
    "]\n",
    "\n",
    "PREDICATES_BENCHES = [\n",
    "    \"transducer_plus\",\n",
    "    \"leetcode\",\n",
    "    \"str_small_rw\",\n",
    "    \"pyex\",\n",
    "    \"full_str_int\",\n",
    "\n",
    "    \"django\",\n",
    "    \"thefuck\",\n",
    "    \"biopython\",\n",
    "    \"notcontains\",\n",
    "    \"selected_hard\",\n",
    "]\n",
    "\n",
    "BENCHMARK_TO_GROUP = {regex_bench:REGEX_GROUP_NAME for regex_bench in REGEX_BENCHES} | {eq_bench:EQUATIONS_GROUP_NAME for eq_bench in EQUATIONS_BENCHES} | {pred_bench:PREDICATES_GROUP_NAME for pred_bench in PREDICATES_BENCHES}\n",
    "\n",
    "BENCHES_GROUPS = {\n",
    "    REGEX_GROUP_NAME: REGEX_BENCHES,\n",
    "    EQUATIONS_GROUP_NAME: EQUATIONS_BENCHES,\n",
    "    PREDICATES_GROUP_NAME: PREDICATES_BENCHES,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = load_benches(BENCHES, TOOLS, bench_selection, BENCHMARK_TO_GROUP)\n",
    "\n",
    "# TODO VBS are ugly for now, will fix it\n",
    "for vbs in VBS:\n",
    "  name = \"+\".join(vbs)\n",
    "  df_all = add_vbs(df_all, vbs, name)\n",
    "  df_all_stats = add_vbs(df_all_stats, vbs, name)\n",
    "  TOOLS.append(name)\n",
    "  # tool_names_mapping[name] = \" + \".join([tool_names_mapping[tool] for tool in vbs])\n",
    "\n",
    "df_all[\"benchmark_group\"] = pd.Categorical(df_all[\"benchmark\"].apply(lambda x: BENCHMARK_TO_GROUP[x]), categories=[EQUATIONS_GROUP_NAME, PREDICATES_GROUP_NAME, REGEX_GROUP_NAME], ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_table(df_all, TOOLS, BENCHES, separately=False))\n",
    "print(simple_table(df_all, TOOLS, BENCHES, separately=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_FOR_COMPARISON = NOODLER\n",
    "# TOOL_FOR_COMPARISON = NOODLER_MODEL\n",
    "# TOOL_FOR_COMPARISON = NOODLER_CHECK_MODEL\n",
    "# TOOL_FOR_COMPARISON ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cactus_plot(df_all, [tool for tool in TOOLS if tool not in [\n",
    "    NOODLER_STATS,\n",
    "    NOODLER_NO_NIELSEN_STATS,\n",
    "    NOODLER_NO_LENGTH_STATS,\n",
    "    NOODLER_NO_MEMB_STATS,\n",
    "    NOODLER_ONLY_STABILIZATION,\n",
    "]], start=int(len(df_all)*0.5), \n",
    "height=3, width=16, put_legend_outside=True, logarithmic_y_axis=True,\n",
    "                  # num_of_x_ticks=6,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in TOOLS:\n",
    "    if tool != TOOL_FOR_COMPARISON:\n",
    "        print(scatter_plot(df_all, TOOL_FOR_COMPARISON, tool, color_column=\"benchmark\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detailed evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if noodler does not return different result than other solvers (i.e. wrong sat/unsat)\n",
    "sanity_check(df_all, TOOL_FOR_COMPARISON, [tool for tool in TOOLS if tool!=TOOL_FOR_COMPARISON])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all formulae where noodler gives different result than sat/unsat/unknown/TO/ERR\n",
    "get_invalid(df_all, TOOL_FOR_COMPARISON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_errors(df_all, TOOL_FOR_COMPARISON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_timeouts(df_all, TOOL_FOR_COMPARISON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unknowns(df_all, TOOL_FOR_COMPARISON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_solved(df_all, TOOL_FOR_COMPARISON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sat(df_all, TOOL_FOR_COMPARISON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unsat(df_all, TOOL_FOR_COMPARISON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: For papers (tables and figures with nicer names or something) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names_mapping = {\n",
    "    NOODLER : \"Z3-Noodler-pos\",\n",
    "    NOODLER_MODEL : \"Z3-Noodlerᴹ\",\n",
    "    CVC5 : \"cvc5\",\n",
    "    CVC5_MODEL : \"cvc5ᴹ\",\n",
    "    Z3 : \"Z3\",\n",
    "    Z3_MODEL : \"Z3ᴹ\",\n",
    "    Z3STR4 : \"Z3stsr4\",\n",
    "    OSTRICH : \"OSTRICH\",\n",
    "    Z3STR3RE : \"Z3str3RE\",\n",
    "    Z3TRAU : \"Z3-Trau\",\n",
    "    OLD_NOODLER : \"Z3-Noodler\",\n",
    "\n",
    "    NOODLER_STATS : \"Z3-Noodler-stats\",\n",
    "    NOODLER_ONLY_STABILIZATION: \"Z3-Noodler-only-stabilization\",\n",
    "    NOODLER_NO_NIELSEN_STATS: \"Z3-Noodler-no-Nielsen-stats\",\n",
    "    NOODLER_NO_LENGTH_STATS: \"Z3-Noodler-no-length-stats\",\n",
    "    NOODLER_NO_MEMB_STATS: \"Z3-Noodler-no-length-stats\",\n",
    "}\n",
    "\n",
    "tool_latex_mapping = {\n",
    "    NOODLER : \"\\\\ziiinoodlerpos\",\n",
    "    CVC5 : \"\\\\cvcv\",\n",
    "    Z3 : \"\\\\ziii\",\n",
    "    NOODLER_MODEL : \"\\\\ziiinoodlermodel\",\n",
    "    CVC5_MODEL : \"\\\\cvcvmodel\",\n",
    "    Z3_MODEL : \"\\\\ziiimodel\",\n",
    "    Z3STR4 : \"\\\\ziiistriv\",\n",
    "    OSTRICH : \"\\\\ostrich\",\n",
    "    Z3STR3RE : \"\\\\ziiistriiire\",\n",
    "    Z3TRAU : \"\\\\ziiitrau\",\n",
    "    OLD_NOODLER : \"\\\\ziiinoodler\",\n",
    "\n",
    "    NOODLER_ONLY_STABILIZATION: \"\\\\ziiinoodleronlystabilization\",\n",
    "    NOODLER_NO_NIELSEN_STATS: \"\\\\ziiinoodlernonielsen\",\n",
    "    NOODLER_NO_LENGTH_STATS: \"\\\\ziiinoodlernolength\",\n",
    "    NOODLER_NO_MEMB_STATS: \"\\\\ziiinoodlernomemb\",\n",
    "}\n",
    "\n",
    "BENCHMARK_TO_LATEX = {\n",
    "    # Benchmark names.\n",
    "    \"sygus_qgen\": \"\\\\sygusqgen\",\n",
    "    \"denghang\": \"\\\\denghang\",\n",
    "    \"automatark\": \"\\\\automatark\",\n",
    "    \"stringfuzz\": \"\\\\stringfuzz\",\n",
    "    \"redos\": \"\\\\redos\",\n",
    "\n",
    "    \"norn\": \"\\\\nornbench\",\n",
    "    \"slog\": \"\\\\slog\",\n",
    "    \"slent\": \"\\\\slent\",\n",
    "    \"omark\": \"\\\\omark\",\n",
    "    \"kepler\": \"\\\\keplerbench\",\n",
    "    \"woorpje\": \"\\\\woorpje\",\n",
    "    \"webapp\": \"\\\\webapp\",\n",
    "    \"kaluza\": \"\\\\kaluza\",\n",
    "\n",
    "    \"transducer_plus\": \"\\\\transducerplus\",\n",
    "    \"leetcode\": \"\\\\leetcode\",\n",
    "    \"str_small_rw\": \"\\\\strsmall\",\n",
    "    \"pyex\": \"\\\\pyex\",\n",
    "    \"full_str_int\": \"\\\\fullstrint\",\n",
    "    \n",
    "    \"django\": \"\\\\django\",\n",
    "    \"biopython\":\"\\\\biopython\",\n",
    "    \"thefuck\": \"\\\\thefuck\",\n",
    "    \"selected_hard\": \"\\\\poshard\",\n",
    "\n",
    "    # Group names.\n",
    "    REGEX_GROUP_NAME: \"\\\\regexbench\",\n",
    "    EQUATIONS_GROUP_NAME: \"\\\\eqbench\",\n",
    "    PREDICATES_GROUP_NAME: \"\\\\predbench\",\n",
    "    \"all\": \"All\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cactus_plot(df_all, [tool for tool in TOOLS if tool not in [\n",
    "    NOODLER_STATS,\n",
    "    NOODLER_NO_NIELSEN_STATS,\n",
    "    NOODLER_NO_LENGTH_STATS,\n",
    "    NOODLER_NO_MEMB_STATS,\n",
    "    NOODLER_ONLY_STABILIZATION,\n",
    "]], start=135700,#int(len(df_all)*0.98), \n",
    "height=3, width=16, put_legend_outside=True, logarithmic_y_axis=True,\n",
    "                  tool_names=tool_names_mapping,\n",
    "                  file_name_to_save=\"cactus\",\n",
    "                  num_of_x_ticks=10,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in TOOLS:\n",
    "    if tool != TOOL_FOR_COMPARISON:\n",
    "        print(scatter_plot(df_all, TOOL_FOR_COMPARISON, tool,\n",
    "                           xname=tool_names_mapping[NOODLER], yname=tool_names_mapping[tool],\n",
    "                           file_name_to_save=f\"{tool_names_mapping[tool]}_vs_{tool_names_mapping[TOOL_FOR_COMPARISON]}\",\n",
    "                           show_legend=False, transparent=True,\n",
    "                           color_column=\"benchmark\"\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_all, index_to_latex=BENCHMARK_TO_LATEX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHES_STATS = [benchmark for benchmark in BENCHES if benchmark not in [\"transducer_plus\"]]\n",
    "df_all_stats = load_benches(BENCHES_STATS, TOOLS_STATS, bench_selection, BENCHMARK_TO_GROUP)\n",
    "df_all_stats[\"benchmark_group\"] = pd.Categorical(df_all_stats[\"benchmark\"].apply(lambda x: BENCHMARK_TO_GROUP[x]), categories=[EQUATIONS_GROUP_NAME, PREDICATES_GROUP_NAME, REGEX_GROUP_NAME], ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solved_time = df_all.copy()\n",
    "\n",
    "df_solved_time.replace({\"benchmark\": {\"snia\": \"kaluza\"}}, inplace=True)\n",
    "\n",
    "for column in df_solved_time.columns:\n",
    "    if column.endswith(\"-runtime\"):\n",
    "        df_solved_time[column] = df_solved_time[column].apply(lambda x: 0 if x == 120 else x)\n",
    "\n",
    "result_columns = []\n",
    "for column in df_solved_time.columns:\n",
    "    if column.endswith(\"-result\"):\n",
    "        result_columns.append(column)\n",
    "replace = {}\n",
    "for result_column in result_columns:\n",
    "    replace[result_column] = {\"sat\": 1, \"unsat\": 1, \"unknown\": 0, \"TO\": 0, \"ERR\": 0, \"unsupported\": 0}\n",
    "df_solved_time.replace(replace, inplace=True)\n",
    "\n",
    "for column in df_solved_time.columns:\n",
    "    if column.endswith(\"-result\"):\n",
    "        df_solved_time[column] = df_solved_time[column].astype(\"Int64\")\n",
    "df_solved_time.drop(columns=[\"name\"], inplace=True)\n",
    "df_solved_time = df_solved_time[[ \\\n",
    "    \"benchmark\", \"benchmark-group\", \\\n",
    "    f\"{NOODLER}-result\", f\"{NOODLER}-runtime\", \\\n",
    "    f\"{NOODLER_MODEL}-result\", f\"{NOODLER_MODEL}-runtime\", \\\n",
    "\n",
    "    f\"{NOODLER_ONLY_STABILIZATION}-result\", f\"{NOODLER_ONLY_STABILIZATION}-runtime\", \\\n",
    "\n",
    "    f\"{NOODLER_NO_NIELSEN_STATS}-result\", f\"{NOODLER_NO_NIELSEN_STATS}-runtime\", \\\n",
    "    f\"{NOODLER_NO_LENGTH_STATS}-result\", f\"{NOODLER_NO_LENGTH_STATS}-runtime\", \\\n",
    "    f\"{NOODLER_NO_MEMB_STATS}-result\", f\"{NOODLER_NO_MEMB_STATS}-runtime\", \\\n",
    "\n",
    "    f\"{CVC5}-result\", f\"{CVC5}-runtime\", \\\n",
    "    f\"{CVC5_MODEL}-result\", f\"{CVC5_MODEL}-runtime\", \\\n",
    "    f\"{Z3}-result\", f\"{Z3}-runtime\", \\\n",
    "    f\"{Z3_MODEL}-result\", f\"{Z3_MODEL}-runtime\", \\\n",
    "]]\n",
    "df_solved_time_per_benchmark = df_solved_time.drop(columns=[\"benchmark-group\"]).groupby(\"benchmark\").sum()\n",
    "\n",
    "df_solved_time_per_group = df_solved_time.drop(columns=[\"benchmark\"]).groupby(\"benchmark-group\").sum()\n",
    "\n",
    "df_solved_time_all = df_solved_time.copy()\n",
    "df_solved_time_all[\"benchmark-group\"] = \"all\"\n",
    "df_solved_time_all = df_solved_time_all.drop(columns=[\"benchmark\"]).groupby(\"benchmark-group\").sum()\n",
    "df_solved_time_per_group_all = pd.concat([df_solved_time_per_group, df_solved_time_all], sort=False)\n",
    "\n",
    "\n",
    "df_solved_time_per_benchmark = solved_time_transpose_per_benchmark(df_solved_time_per_benchmark)\n",
    "df_solved_time_per_group = solved_time_transpose_per_benchmark(df_solved_time_per_group)\n",
    "df_solved_time_per_group_all = solved_time_transpose_per_benchmark(df_solved_time_per_group_all)\n",
    "\n",
    "# Solved-time table for regex and predicates benchmarks.\n",
    "regex_predicates_benches = [benchmark for benchmark in REGEX_BENCHES + PREDICATES_BENCHES if benchmark not in [\"transducer_plus\"]]\n",
    "regex_predicates_benches_columns = []\n",
    "for bench in regex_predicates_benches:\n",
    "    regex_predicates_benches_columns.append(f\"{bench}-result\")\n",
    "    regex_predicates_benches_columns.append(f\"{bench}-runtime\")\n",
    "table_lines = table_solved_time(df_solved_time_per_benchmark[regex_predicates_benches_columns], df_all, regex_predicates_benches, BENCHMARK_TO_LATEX, tool_latex_mapping)\n",
    "\n",
    "table_lines.insert(0, \"\\\\begin{booktabs}{colspec={lQ[r,gray!30]Q[r,gray!30]rrQ[r,gray!30]Q[r,gray!30]rrQ[r,gray!30]Q[r,gray!30]rrQ[r,gray!30]Q[r,gray!30]rrQ[r,gray!30]Q[r,gray!30]},rowsep=0pt,row{5-6}={yellow},row{1}={white},cell{2,3}{even}={c=2}{c}}\")\n",
    "table_lines.insert(2, \"& \\\\SetCell[c=10]{c} \\\\regexbench &&&&&&&&&& \\\\SetCell[c=8]{c} \\\\predbench &&&&&&&\\\\\\\\\\\\cmidrule[lr]{2-11}\\\\cmidrule[lr]{12-19}\")\n",
    "table_lines.insert(len(table_lines), \"\\\\end{booktabs}\")\n",
    "with open(\"sum_solved_result_per_tool_for_benches_regex_predicates.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(table_lines))\n",
    "\n",
    "# Solved-time table for equations benchmarks.\n",
    "equations_benches_columns = []\n",
    "for bench in EQUATIONS_BENCHES:\n",
    "    equations_benches_columns.append(f\"{bench}-result\")\n",
    "    equations_benches_columns.append(f\"{bench}-runtime\")\n",
    "table_lines = table_solved_time(df_solved_time_per_benchmark[equations_benches_columns], df_all, EQUATIONS_BENCHES, BENCHMARK_TO_LATEX, tool_latex_mapping)\n",
    "table_lines.insert(0, \"\\\\begin{booktabs}{colspec={lQ[r,gray!30]Q[r,gray!30]rrQ[r,gray!30]Q[r,gray!30]rrQ[r,gray!30]Q[r,gray!30]rrQ[r,gray!30]Q[r,gray!30]rr},rowsep=0pt,row{5-6}={yellow},row{1}={white},cell{2,3}{even}={c=2}{c}}\")\n",
    "table_lines.insert(2, \"& \\\\SetCell[c=16]{c} \\\\eqbench &&&&&&&&&&&&&&&\\\\\\\\\\\\cmidrule[lr]{2-17}\")\n",
    "table_lines.insert(len(table_lines), \"\\\\end{booktabs}\")\n",
    "with open(\"sum_solved_result_per_tool_for_benches_equations.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(table_lines))\n",
    "\n",
    "\n",
    "# Solved-time table for benchmark groups.\n",
    "benchmark_group_names = list(BENCHES_GROUPS.keys()) + [\"all\"]\n",
    "benches_groups_columns = []\n",
    "for bench in benchmark_group_names:\n",
    "    benches_groups_columns.append(f\"{bench}-result\")\n",
    "    benches_groups_columns.append(f\"{bench}-runtime\")\n",
    "table_lines = table_solved_time(df_solved_time_per_group_all[benches_groups_columns], df_all, benchmark_group_names, BENCHMARK_TO_LATEX, tool_latex_mapping, per_column=\"benchmark-group\")\n",
    "table_lines.insert(0, \"\\\\begin{booktabs}{colspec={lQ[r,gray!30]Q[r,gray!30]rrQ[r,gray!30]Q[r,gray!30]rr},rowsep=0pt,row{4-5}={yellow},cell{1,2}{even}={c=2}{c}}\")\n",
    "# table_lines.insert(2, \"& \\\\SetCell[c=2]{c} \\\\regexbench & & \\\\SetCell[c=2]{c} \\\\eqbench & & \\\\SetCell[c=2]{c} \\\\predbench & & \\\\SetCell[c=2]{c} All & \\\\\\\\\\\\cmidrule[lr]{2-9}\")\n",
    "table_lines.insert(len(table_lines), \"\\\\end{booktabs}\")\n",
    "with open(\"sum_solved_result_per_tool_for_benchmark_groups.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(table_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noodler Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = []\n",
    "for key in BENCHMARK_TO_GROUP.keys():\n",
    "    if key == \"transducer_plus\":\n",
    "        continue\n",
    "    if key not in order:\n",
    "        order.append(key)\n",
    "for value in BENCHMARK_TO_GROUP.values():\n",
    "    if value not in order:\n",
    "        order.append(value)\n",
    "df_stats, df_stats_zeroed_nans = get_stats_dfs(df_all_stats, NOODLER_STATS, order)\n",
    "df_stats_groups = group_to_benchmark_groups(df_stats, BENCHMARK_TO_GROUP, order)\n",
    "df_stats_groups_zeroed_nans = group_to_benchmark_groups(df_stats_zeroed_nans, BENCHMARK_TO_GROUP, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_zeroed_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_grouped_by_benchmark(df_stats_zeroed_nans, NOODLER_STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_grouped_by_benchmark(df_stats_groups_zeroed_nans, NOODLER_STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_grouped_by_benchmark_counts(df_stats, NOODLER_STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_grouped_by_benchmark_counts(df_stats_groups, NOODLER_STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_total(df_stats_zeroed_nans, NOODLER_STATS, BENCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_per_benchmark_sum = get_stats_per_benchmark_paper(df_stats_zeroed_nans)\n",
    "df_stats_per_benchmark_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_per_benchmark.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_per_benchmark_sum, index_to_latex=BENCHMARK_TO_LATEX)[4:-2]))\n",
    "with open(\"stats_per_benchmark_no_preprocess.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_per_benchmark_sum.drop([benchmark for benchmark in df_stats_per_benchmark_sum.keys() if benchmark.endswith(\"-preprocess\")], axis='columns'), index_to_latex=BENCHMARK_TO_LATEX)[4:-2]))\n",
    "with open(\"stats_per_benchmark_no_preprocess_percents.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_per_benchmark_sum.drop([benchmark for benchmark in df_stats_per_benchmark_sum.keys() if benchmark.endswith(\"-preprocess\")], axis='columns'),index_to_latex=BENCHMARK_TO_LATEX, float_format=\"{:.2f}\\\\,\\\\%\")[4:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_per_group_sum = get_stats_per_benchmark_paper(df_stats_groups_zeroed_nans)\n",
    "df_stats_per_group_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_per_group.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_per_group_sum, index_to_latex=BENCHMARK_TO_LATEX, float_format=\"{:.2f}\")[4:-2]))\n",
    "\n",
    "with open(\"stats_per_group_no_preprocess_no_preprocess.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_per_group_sum.drop([benchmark for benchmark in df_stats_per_group_sum.keys() if benchmark.endswith(\"-preprocess\")], axis='columns'),index_to_latex=BENCHMARK_TO_LATEX)[4:-2]))\n",
    "\n",
    "with open(\"stats_per_group_no_preprocess_percents.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_per_benchmark_sum.drop([benchmark for benchmark in df_stats_per_benchmark_sum.keys() if benchmark.endswith(\"-preprocess\")], axis='columns'),index_to_latex=BENCHMARK_TO_LATEX, float_format=\"{:.2f}\\\\,\\\\%\")[4:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_total_sum = get_stats_per_benchmark_paper(group_to_benchmark_groups(df_stats_zeroed_nans, lambda _: \"total\"))\n",
    "df_stats_total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_groups_total = pd.concat([df_stats_per_group_sum, df_stats_total_sum])\n",
    "df_stats_groups_total = df_stats_groups_total.drop(\"noodler-final_checks\", axis='columns')\n",
    "df_stats_groups_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_per_group_total.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_groups_total, index_to_latex=BENCHMARK_TO_LATEX)[4:-2]))\n",
    "\n",
    "with open(\"stats_per_group_total_no_preprocess.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_groups_total.drop([benchmark for benchmark in df_stats_per_group_sum.keys() if benchmark.endswith(\"-preprocess\")], axis='columns'), index_to_latex=BENCHMARK_TO_LATEX)[4:-2]))\n",
    "\n",
    "with open(\"stats_per_group_total_no_preprocess_percents.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_groups_total.drop([benchmark for benchmark in df_stats_groups_total.keys() if benchmark.endswith(\"-preprocess\")], axis='columns'), index_to_latex=BENCHMARK_TO_LATEX, float_format=\"{:.2f}\\\\,\\\\%\")[4:-2]))\n",
    "\n",
    "df_stats_groups_total_transposed = df_stats_groups_total.transpose()\n",
    "\n",
    "with open(\"stats_per_group_total_transposed.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_groups_total_transposed, index_to_latex=BENCHMARK_TO_LATEX, format_index_name=False)[3:-2]))\n",
    "\n",
    "df_stats_groups_total_transposed = df_stats_groups_total.drop([benchmark for benchmark in df_stats_groups_total if benchmark.endswith(\"-preprocess\")], axis='columns').transpose()\n",
    "\n",
    "with open(\"stats_per_group_total_no_preprocess_transposed.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_groups_total_transposed, index_to_latex=BENCHMARK_TO_LATEX, format_index_name=False)[3:-2]))\n",
    "\n",
    "with open(\"stats_per_group_total_no_preprocess_percents_transposed.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_stats_groups_total_transposed, index_to_latex=BENCHMARK_TO_LATEX, float_format=\"{:.2f}\\\\,\\\\%\", format_index_name=False)[3:-2]))\n",
    "\n",
    "df_stats_groups_total_special_transposed = pd.DataFrame(columns=list(df_stats_groups_total_transposed.keys()))\n",
    "\n",
    "concat_rows = []\n",
    "for index, _ in df_stats_groups_total_transposed.iterrows():\n",
    "    if not index.endswith(\"-start\"):\n",
    "        continue\n",
    "\n",
    "    index_start_name = index\n",
    "    index_finish_name = index_start_name.replace(\"-start\", \"-finish\")\n",
    "    procedure_name = index_start_name.replace(\"-start\", \"\")\n",
    "    start_row = df_stats_groups_total_transposed.loc[[index_start_name]]\n",
    "    finish_row = df_stats_groups_total_transposed.loc[[index_finish_name]]\n",
    "\n",
    "    concat_row = [procedure_name]\n",
    "    values_start = list(start_row.values)\n",
    "    values_finish = list(finish_row.values)\n",
    "    for val_start, val_finish in zip(values_start, values_finish):\n",
    "        for val_start, val_finish in zip(val_start, val_finish):\n",
    "            concat_row += [val_start, val_finish]\n",
    "\n",
    "    concat_rows.append(concat_row)\n",
    "\n",
    "columns = [\"procedure\"]\n",
    "for column in df_stats_groups_total_transposed.keys():\n",
    "    columns.append(f\"{column}-start\")\n",
    "    columns.append(f\"{column}-finish\")\n",
    "df_concat_rows = pd.DataFrame(concat_rows, columns=columns)\n",
    "df_concat_rows.set_index(\"procedure\", inplace=True)\n",
    "\n",
    "with open(\"stats_per_group_total_special_transposed.tex\", \"w+\") as f:\n",
    "    f.write('\\n'.join(write_latex_table_body(df_concat_rows, index_to_latex=BENCHMARK_TO_LATEX)[4:-2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
